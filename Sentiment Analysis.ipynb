{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this works\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU\n",
    "print(\"this works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Komodo', 'Dragon', 'is', 'native', 'to', 'Southern', 'Indonesia', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "indiv_words = word_tokenize(\"The Komodo Dragon is native to Southern Indonesia.\")\n",
    "print(indiv_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Komodo', 'Dragon', 'native', 'Southern', 'Indonesia', '.']\n"
     ]
    }
   ],
   "source": [
    "#stopwords removes a lot of the unnecessary words (like a, an, is, to etc.)\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "unwanted_words = set(stopwords.words('english'))\n",
    "important_tokens = [x for x in indiv_words if not x in unwanted_words]\n",
    "print(important_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing the data set\n",
    "imdb = keras.datasets.imdb\n",
    "\n",
    "#the code below was added to solve pickle error\n",
    "# save np.load\n",
    "np_load_prev = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_prev(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_prev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: 25000, labels: 25000\n"
     ]
    }
   ],
   "source": [
    "print(\"inputs: {}, labels: {}\".format(len(x_train), len(y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34701\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "print(word_index.get(\"fawn\", \"none\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    }
   ],
   "source": [
    "max_len = max(len(x_train[i]) for i in x_train)\n",
    "print(max_len)\n",
    "#this is a problem. We need to set a max_len\n",
    "#we could alternatively use one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458 4468   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172 4536 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920 4613  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      " 5244   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117 5952   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194 7486   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30 5535   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16 4472  113\n",
      "  103   32   15   16 5345   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "1\n",
      "[   1  591  202   14   31    6  717   10   10    2    2    5    4  360\n",
      "    7    4  177 5760  394  354    4  123    9 1035 1035 1035   10   10\n",
      "   13   92  124   89  488 7944  100   28 1668   14   31   23   27 7479\n",
      "   29  220  468    8  124   14  286  170    8  157   46    5   27  239\n",
      "   16  179    2   38   32   25 7944  451  202   14    6  717    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=256, value=0, padding='post')\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=256, value=0, padding='post')\n",
    "print(x_test[0])\n",
    "print(y_test[0])\n",
    "#now we can set the tensor input size to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model!!!\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(10000, 100, input_length=256))\n",
    "model.add(GRU(units=32, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/123\n",
      "25000/25000 [==============================] - 37s 1ms/step - loss: 0.6881 - acc: 0.5239 - val_loss: 0.6753 - val_acc: 0.5521\n",
      "Epoch 2/123\n",
      "25000/25000 [==============================] - 36s 1ms/step - loss: 0.6437 - acc: 0.5818 - val_loss: 0.6394 - val_acc: 0.5824\n",
      "Epoch 3/123\n",
      "25000/25000 [==============================] - 36s 1ms/step - loss: 0.6020 - acc: 0.6178 - val_loss: 0.6364 - val_acc: 0.5884\n",
      "Epoch 4/123\n",
      "25000/25000 [==============================] - 36s 1ms/step - loss: 0.5654 - acc: 0.6767 - val_loss: 0.6342 - val_acc: 0.5956\n",
      "Epoch 5/123\n",
      "25000/25000 [==============================] - 35s 1ms/step - loss: 0.5220 - acc: 0.7524 - val_loss: 0.4642 - val_acc: 0.8114\n",
      "Epoch 6/123\n",
      "25000/25000 [==============================] - 38s 2ms/step - loss: 0.4605 - acc: 0.7982 - val_loss: 0.5065 - val_acc: 0.7662\n",
      "Epoch 7/123\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.4284 - acc: 0.8228 - val_loss: 0.4809 - val_acc: 0.8044\n",
      "Epoch 8/123\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.4660 - acc: 0.7908 - val_loss: 0.4478 - val_acc: 0.8071\n",
      "Epoch 9/123\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.3982 - acc: 0.8368 - val_loss: 0.4406 - val_acc: 0.8194\n",
      "Epoch 10/123\n",
      "25000/25000 [==============================] - 38s 2ms/step - loss: 0.3711 - acc: 0.8516 - val_loss: 0.4285 - val_acc: 0.8237\n",
      "Epoch 11/123\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.3534 - acc: 0.8600 - val_loss: 0.4322 - val_acc: 0.8244\n",
      "Epoch 12/123\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.3313 - acc: 0.8691 - val_loss: 0.4264 - val_acc: 0.8250\n",
      "Epoch 13/123\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.3317 - acc: 0.8678 - val_loss: 0.4025 - val_acc: 0.8382\n",
      "Epoch 14/123\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.3036 - acc: 0.8768 - val_loss: 0.4346 - val_acc: 0.8296\n",
      "Epoch 15/123\n",
      "25000/25000 [==============================] - 42s 2ms/step - loss: 0.2929 - acc: 0.8817 - val_loss: 0.4131 - val_acc: 0.8307\n",
      "Epoch 16/123\n",
      "25000/25000 [==============================] - 43s 2ms/step - loss: 0.2972 - acc: 0.8832 - val_loss: 0.4313 - val_acc: 0.8247\n",
      "Epoch 17/123\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.2876 - acc: 0.8880 - val_loss: 0.4234 - val_acc: 0.8318\n",
      "Epoch 18/123\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.3028 - acc: 0.8784 - val_loss: 0.4284 - val_acc: 0.8222\n",
      "Epoch 19/123\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.2719 - acc: 0.8948 - val_loss: 0.4043 - val_acc: 0.8366\n",
      "Epoch 20/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.2875 - acc: 0.8836 - val_loss: 0.4477 - val_acc: 0.8225\n",
      "Epoch 21/123\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.2484 - acc: 0.9067 - val_loss: 0.4121 - val_acc: 0.8338\n",
      "Epoch 22/123\n",
      "25000/25000 [==============================] - 40s 2ms/step - loss: 0.2385 - acc: 0.9088 - val_loss: 0.4212 - val_acc: 0.8314\n",
      "Epoch 23/123\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.2270 - acc: 0.9130 - val_loss: 0.4176 - val_acc: 0.8356\n",
      "Epoch 24/123\n",
      "25000/25000 [==============================] - 43s 2ms/step - loss: 0.2731 - acc: 0.8935 - val_loss: 0.4635 - val_acc: 0.8083\n",
      "Epoch 25/123\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.2508 - acc: 0.9004 - val_loss: 0.4368 - val_acc: 0.8153\n",
      "Epoch 26/123\n",
      "25000/25000 [==============================] - 42s 2ms/step - loss: 0.2321 - acc: 0.9072 - val_loss: 0.4141 - val_acc: 0.8329\n",
      "Epoch 27/123\n",
      "25000/25000 [==============================] - 43s 2ms/step - loss: 0.2306 - acc: 0.9128 - val_loss: 0.4316 - val_acc: 0.8318\n",
      "Epoch 28/123\n",
      "25000/25000 [==============================] - 43s 2ms/step - loss: 0.2120 - acc: 0.9202 - val_loss: 0.4347 - val_acc: 0.8298\n",
      "Epoch 29/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.2174 - acc: 0.9182 - val_loss: 0.4433 - val_acc: 0.8269\n",
      "Epoch 30/123\n",
      "25000/25000 [==============================] - 42s 2ms/step - loss: 0.2038 - acc: 0.9244 - val_loss: 0.4531 - val_acc: 0.8254\n",
      "Epoch 31/123\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.3419 - acc: 0.8576 - val_loss: 0.4514 - val_acc: 0.8322\n",
      "Epoch 32/123\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.2232 - acc: 0.9137 - val_loss: 0.4384 - val_acc: 0.8308\n",
      "Epoch 33/123\n",
      "25000/25000 [==============================] - 42s 2ms/step - loss: 0.2099 - acc: 0.9184 - val_loss: 0.4491 - val_acc: 0.8326\n",
      "Epoch 34/123\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.2029 - acc: 0.9235 - val_loss: 0.4476 - val_acc: 0.8284\n",
      "Epoch 35/123\n",
      "25000/25000 [==============================] - 43s 2ms/step - loss: 0.1991 - acc: 0.9234 - val_loss: 0.4638 - val_acc: 0.8275\n",
      "Epoch 36/123\n",
      "25000/25000 [==============================] - 43s 2ms/step - loss: 0.1869 - acc: 0.9300 - val_loss: 0.4608 - val_acc: 0.8290\n",
      "Epoch 37/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.1841 - acc: 0.9304 - val_loss: 0.4704 - val_acc: 0.8274\n",
      "Epoch 38/123\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.1795 - acc: 0.9327 - val_loss: 0.4701 - val_acc: 0.8275\n",
      "Epoch 39/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.1750 - acc: 0.9343 - val_loss: 0.4704 - val_acc: 0.8291\n",
      "Epoch 40/123\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.1668 - acc: 0.9382 - val_loss: 0.4765 - val_acc: 0.8295\n",
      "Epoch 41/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1633 - acc: 0.9380 - val_loss: 0.4690 - val_acc: 0.8288\n",
      "Epoch 42/123\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.1651 - acc: 0.9378 - val_loss: 0.4843 - val_acc: 0.8242\n",
      "Epoch 43/123\n",
      "25000/25000 [==============================] - 50s 2ms/step - loss: 0.1561 - acc: 0.9412 - val_loss: 0.4592 - val_acc: 0.8282\n",
      "Epoch 44/123\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.1545 - acc: 0.9435 - val_loss: 0.4838 - val_acc: 0.8320\n",
      "Epoch 45/123\n",
      "25000/25000 [==============================] - 53s 2ms/step - loss: 0.1573 - acc: 0.9407 - val_loss: 0.4845 - val_acc: 0.8311\n",
      "Epoch 46/123\n",
      "25000/25000 [==============================] - 59s 2ms/step - loss: 0.1559 - acc: 0.9401 - val_loss: 0.5178 - val_acc: 0.8264\n",
      "Epoch 47/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1457 - acc: 0.9470 - val_loss: 0.5035 - val_acc: 0.8310\n",
      "Epoch 48/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1412 - acc: 0.9486 - val_loss: 0.5031 - val_acc: 0.8311\n",
      "Epoch 49/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.1402 - acc: 0.9471 - val_loss: 0.5045 - val_acc: 0.8276\n",
      "Epoch 50/123\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.1327 - acc: 0.9516 - val_loss: 0.5217 - val_acc: 0.8296\n",
      "Epoch 51/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1272 - acc: 0.9520 - val_loss: 0.5446 - val_acc: 0.8295\n",
      "Epoch 52/123\n",
      "25000/25000 [==============================] - 43s 2ms/step - loss: 0.1223 - acc: 0.9547 - val_loss: 0.5411 - val_acc: 0.8297\n",
      "Epoch 53/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1245 - acc: 0.9552 - val_loss: 0.5590 - val_acc: 0.8269\n",
      "Epoch 54/123\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.1219 - acc: 0.9552 - val_loss: 0.5481 - val_acc: 0.8293\n",
      "Epoch 55/123\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.1246 - acc: 0.9548 - val_loss: 0.5480 - val_acc: 0.8280\n",
      "Epoch 56/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.1274 - acc: 0.9549 - val_loss: 0.5393 - val_acc: 0.8277\n",
      "Epoch 57/123\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.1184 - acc: 0.9569 - val_loss: 0.5247 - val_acc: 0.8283\n",
      "Epoch 58/123\n",
      "25000/25000 [==============================] - 54s 2ms/step - loss: 0.1136 - acc: 0.9603 - val_loss: 0.5497 - val_acc: 0.8294\n",
      "Epoch 59/123\n",
      "25000/25000 [==============================] - 50s 2ms/step - loss: 0.1137 - acc: 0.9604 - val_loss: 0.5543 - val_acc: 0.8300\n",
      "Epoch 60/123\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.1132 - acc: 0.9596 - val_loss: 0.5766 - val_acc: 0.8285\n",
      "Epoch 61/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1219 - acc: 0.9560 - val_loss: 0.5589 - val_acc: 0.8305\n",
      "Epoch 62/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1046 - acc: 0.9637 - val_loss: 0.5709 - val_acc: 0.8329\n",
      "Epoch 63/123\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.1436 - acc: 0.9458 - val_loss: 0.4502 - val_acc: 0.8205\n",
      "Epoch 64/123\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.1329 - acc: 0.9532 - val_loss: 0.5213 - val_acc: 0.8327\n",
      "Epoch 65/123\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.1087 - acc: 0.9622 - val_loss: 0.5794 - val_acc: 0.8236\n",
      "Epoch 66/123\n",
      "25000/25000 [==============================] - 50s 2ms/step - loss: 0.1404 - acc: 0.9489 - val_loss: 0.5149 - val_acc: 0.8336\n",
      "Epoch 67/123\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.1093 - acc: 0.9630 - val_loss: 0.5573 - val_acc: 0.8302\n",
      "Epoch 68/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1051 - acc: 0.9630 - val_loss: 0.5506 - val_acc: 0.8298\n",
      "Epoch 69/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.1000 - acc: 0.9650 - val_loss: 0.5788 - val_acc: 0.8302\n",
      "Epoch 70/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0963 - acc: 0.9662 - val_loss: 0.6008 - val_acc: 0.8312\n",
      "Epoch 71/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0905 - acc: 0.9699 - val_loss: 0.5992 - val_acc: 0.8310\n",
      "Epoch 72/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0848 - acc: 0.9713 - val_loss: 0.6079 - val_acc: 0.8306\n",
      "Epoch 73/123\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.0844 - acc: 0.9708 - val_loss: 0.6218 - val_acc: 0.8256\n",
      "Epoch 74/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0908 - acc: 0.9683 - val_loss: 0.6410 - val_acc: 0.8296\n",
      "Epoch 75/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0800 - acc: 0.9727 - val_loss: 0.6251 - val_acc: 0.8306\n",
      "Epoch 76/123\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.0750 - acc: 0.9738 - val_loss: 0.6304 - val_acc: 0.8266\n",
      "Epoch 77/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0772 - acc: 0.9739 - val_loss: 0.6668 - val_acc: 0.8323\n",
      "Epoch 78/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.1998 - acc: 0.9265 - val_loss: 0.6709 - val_acc: 0.8061\n",
      "Epoch 79/123\n",
      "25000/25000 [==============================] - 50s 2ms/step - loss: 0.1091 - acc: 0.9611 - val_loss: 0.6252 - val_acc: 0.8244\n",
      "Epoch 80/123\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.0976 - acc: 0.9662 - val_loss: 0.5900 - val_acc: 0.8230\n",
      "Epoch 81/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0941 - acc: 0.9686 - val_loss: 0.6172 - val_acc: 0.8264\n",
      "Epoch 82/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0815 - acc: 0.9725 - val_loss: 0.6437 - val_acc: 0.8253\n",
      "Epoch 83/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0797 - acc: 0.9727 - val_loss: 0.6330 - val_acc: 0.8266\n",
      "Epoch 84/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0861 - acc: 0.9709 - val_loss: 0.6479 - val_acc: 0.8250\n",
      "Epoch 85/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0765 - acc: 0.9737 - val_loss: 0.6174 - val_acc: 0.8260\n",
      "Epoch 86/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0701 - acc: 0.9772 - val_loss: 0.6366 - val_acc: 0.8306\n",
      "Epoch 87/123\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.0682 - acc: 0.9772 - val_loss: 0.6527 - val_acc: 0.8296\n",
      "Epoch 88/123\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.0672 - acc: 0.9782 - val_loss: 0.6556 - val_acc: 0.8301\n",
      "Epoch 89/123\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.0660 - acc: 0.9788 - val_loss: 0.6755 - val_acc: 0.8294\n",
      "Epoch 90/123\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.0694 - acc: 0.9772 - val_loss: 0.6665 - val_acc: 0.8307\n",
      "Epoch 91/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0638 - acc: 0.9785 - val_loss: 0.6740 - val_acc: 0.8320\n",
      "Epoch 92/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0592 - acc: 0.9802 - val_loss: 0.6601 - val_acc: 0.8300\n",
      "Epoch 93/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0598 - acc: 0.9811 - val_loss: 0.6668 - val_acc: 0.8296\n",
      "Epoch 94/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0615 - acc: 0.9800 - val_loss: 0.7017 - val_acc: 0.8289\n",
      "Epoch 95/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0668 - acc: 0.9779 - val_loss: 0.6958 - val_acc: 0.8327\n",
      "Epoch 96/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0759 - acc: 0.9736 - val_loss: 0.6848 - val_acc: 0.8283\n",
      "Epoch 97/123\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.1493 - acc: 0.9464 - val_loss: 0.5555 - val_acc: 0.7917\n",
      "Epoch 98/123\n",
      "25000/25000 [==============================] - 49s 2ms/step - loss: 0.1444 - acc: 0.9462 - val_loss: 0.5584 - val_acc: 0.8313\n",
      "Epoch 99/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.1002 - acc: 0.9644 - val_loss: 0.5992 - val_acc: 0.8306\n",
      "Epoch 100/123\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 0.0814 - acc: 0.9718 - val_loss: 0.6289 - val_acc: 0.8308\n",
      "Epoch 101/123\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.0725 - acc: 0.9748 - val_loss: 0.6572 - val_acc: 0.8302\n",
      "Epoch 102/123\n",
      "25000/25000 [==============================] - 50s 2ms/step - loss: 0.0811 - acc: 0.9715 - val_loss: 0.6141 - val_acc: 0.8316\n",
      "Epoch 103/123\n",
      "25000/25000 [==============================] - 47s 2ms/step - loss: 0.0813 - acc: 0.9712 - val_loss: 0.6321 - val_acc: 0.8286\n",
      "Epoch 104/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0743 - acc: 0.9746 - val_loss: 0.6512 - val_acc: 0.8282\n",
      "Epoch 105/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0627 - acc: 0.9792 - val_loss: 0.6737 - val_acc: 0.8310\n",
      "Epoch 106/123\n",
      "25000/25000 [==============================] - 51s 2ms/step - loss: 0.0645 - acc: 0.9775 - val_loss: 0.6808 - val_acc: 0.8288\n",
      "Epoch 107/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0601 - acc: 0.9802 - val_loss: 0.6901 - val_acc: 0.8282\n",
      "Epoch 108/123\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.0564 - acc: 0.9810 - val_loss: 0.6724 - val_acc: 0.8301\n",
      "Epoch 109/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0530 - acc: 0.9827 - val_loss: 0.6980 - val_acc: 0.8290\n",
      "Epoch 110/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0588 - acc: 0.9806 - val_loss: 0.7001 - val_acc: 0.8290\n",
      "Epoch 111/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0524 - acc: 0.9826 - val_loss: 0.6875 - val_acc: 0.8285\n",
      "Epoch 112/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0516 - acc: 0.9833 - val_loss: 0.7421 - val_acc: 0.8270\n",
      "Epoch 113/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0464 - acc: 0.9852 - val_loss: 0.7374 - val_acc: 0.8279\n",
      "Epoch 114/123\n",
      "25000/25000 [==============================] - 48s 2ms/step - loss: 0.0456 - acc: 0.9851 - val_loss: 0.7279 - val_acc: 0.8258\n",
      "Epoch 115/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0426 - acc: 0.9863 - val_loss: 0.7519 - val_acc: 0.8261\n",
      "Epoch 116/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0446 - acc: 0.9858 - val_loss: 0.7599 - val_acc: 0.8273\n",
      "Epoch 117/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0381 - acc: 0.9882 - val_loss: 0.7886 - val_acc: 0.8289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0655 - acc: 0.9783 - val_loss: 0.7291 - val_acc: 0.8142\n",
      "Epoch 119/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0451 - acc: 0.9855 - val_loss: 0.8082 - val_acc: 0.8254\n",
      "Epoch 120/123\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.0389 - acc: 0.9876 - val_loss: 0.7830 - val_acc: 0.8273\n",
      "Epoch 121/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0387 - acc: 0.9880 - val_loss: 0.7870 - val_acc: 0.8278\n",
      "Epoch 122/123\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.0381 - acc: 0.9879 - val_loss: 0.8143 - val_acc: 0.8254\n",
      "Epoch 123/123\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.0383 - acc: 0.9879 - val_loss: 0.7876 - val_acc: 0.8264\n"
     ]
    }
   ],
   "source": [
    "#training!\n",
    "model.fit(x_train, y_train, epochs=123, batch_size=256, validation_data=(x_test, y_test), verbose=1)\n",
    "model.save(\"my_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 36s 1ms/step\n",
      "[0.7876195859146118, 0.8264]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model \n",
    "model = load_model('my_model1.h5')\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type in sentence.It is impossible to hate this movie\n",
      "['It', 'is', 'impossible', 'to', 'hate', 'this', 'movie']\n",
      "[[   6 1164    5  781   11   17    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9734572]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "imdb = keras.datasets.imdb\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "sentence = input(\"Type in sentence.\")\n",
    "type(sentence)\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "testIntArray = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word in word_index:\n",
    "        testIntArray.append(word_index[word])\n",
    "    \n",
    "\n",
    "finalTestArray = np.array([testIntArray])\n",
    "finalTestArray = keras.preprocessing.sequence.pad_sequences(finalTestArray, maxlen=256, value=0, padding='post')\n",
    "print(finalTestArray)\n",
    "\n",
    "model.predict(finalTestArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
