{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported model related libs\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, GRU, Activation, Flatten\n",
    "print(\"Imported model related libs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported word embedding/misc libs\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re  #regular expression for data pre processing\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, conll2000\n",
    "print(\"Imported word embedding/misc libs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "25000\n",
      "428\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath = \"C:/Users/varun/Desktop/Work/reviews/train\"\n",
    "pos_revs = []\n",
    "neg_revs = []\n",
    "\n",
    "def get_review_info(file_path):\n",
    "    onlyfiles = []\n",
    "    for f in listdir(file_path):\n",
    "        if os.path.isfile(os.path.join(file_path, f)):\n",
    "            onlyfiles.append(f)\n",
    "    return onlyfiles\n",
    "            \n",
    "pos_revs = get_review_info(mypath + \"/pos\")\n",
    "neg_revs = get_review_info(mypath + \"/neg\")\n",
    "\n",
    "assert len(neg_revs) == 12500, \"ERROR: No. of negative reviews not 12500\"\n",
    "assert len(pos_revs) == 12500, \"ERROR: No. of positive reviews not 12500\"\n",
    "print(\"success\")\n",
    "    \n",
    "    \n",
    "def load_dataset(file_loc):\n",
    "    files = file_loc + \"/pos\"\n",
    "    all_reviews = []\n",
    "    tokens = []\n",
    "    y_train = []\n",
    "    for x in range(0,2):\n",
    "        for file in listdir(files):\n",
    "            f = open(files + \"/\" + file, encoding=\"utf8\")\n",
    "            for word in f.read().split():\n",
    "                tokens.append(word)\n",
    "            all_reviews.append(tokens)\n",
    "            tokens = []\n",
    "        files = file_loc + \"/neg\"\n",
    "    \n",
    "    print(len(all_reviews))\n",
    "    assert len(all_reviews) == 25000, \"ERROR: Total number of reviews does not add up to 25000...\"\n",
    "    return all_reviews\n",
    "\n",
    "list = load_dataset(mypath) #list of text samples\n",
    "\n",
    "#for item in list[:5]:\n",
    " #   print(item)\n",
    "\n",
    "print(len(list[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join('', 'word2vec_embedding_imdb_dataset.txt'), encoding = \"utf-8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "#print(embeddings_index[\"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280617\n"
     ]
    }
   ],
   "source": [
    "#Training the word2vec model\n",
    "w2vmodel = Word2Vec(list, size = 128, min_count = 1,window = 3, sg = 1) #we want to use skip-gram (takes in a word, guesses\n",
    "                                                                        #the words around it)\n",
    "print(len(w2vmodel.wv.vocab))\n",
    "filename = \"word2vec_embedding_imdb_dataset.txt\"\n",
    "w2vmodel.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('freaky', 0.840491771697998),\n",
       " ('mad', 0.8108929395675659),\n",
       " ('rapping', 0.8106375932693481),\n",
       " ('demon', 0.800931453704834),\n",
       " ('deformed', 0.8001381158828735),\n",
       " ('fat,', 0.8000245094299316),\n",
       " ('bald', 0.7986127138137817),\n",
       " ('friendly', 0.7966055870056152),\n",
       " ('clown', 0.794162392616272),\n",
       " ('weird', 0.7928417325019836)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel.wv.most_similar('crazy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varun\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.15717083,  0.00719881, -0.13486277,  0.24161513,  0.5917264 ,\n",
       "       -0.38161474,  0.0981496 ,  0.06785046, -0.13178508, -0.2610271 ,\n",
       "        0.36278152, -0.74105865,  0.02209078,  0.5998116 ,  0.11493874,\n",
       "       -0.36380413,  0.33248094, -0.2831874 , -0.32796293, -0.5830784 ,\n",
       "       -0.2930365 , -0.06944399, -1.1124429 ,  0.6153449 ,  0.16190404,\n",
       "        0.01954959, -0.08898558, -0.13666491, -0.13935591, -0.4815852 ,\n",
       "        0.543503  ,  0.04405535, -0.2948441 ,  0.38509563,  0.95225906,\n",
       "       -0.16870022,  0.39097536,  0.30480352,  0.80282307,  0.8503552 ,\n",
       "        0.36540058,  0.86473316,  0.17450067,  0.2526917 ,  0.26038042,\n",
       "        0.04001397,  0.1455762 ,  0.23201808, -0.6949306 , -0.3180783 ,\n",
       "        0.33362773,  0.34399712, -0.41319567,  0.41418356,  0.13962907,\n",
       "        0.25074008, -0.10245742, -0.7726181 , -0.24751253, -0.2635865 ,\n",
       "        0.05062889, -0.53740066,  1.0597956 ,  0.5728781 ,  0.02128099,\n",
       "       -0.09101702, -0.42029145,  0.23381889, -0.03530229,  0.36857697,\n",
       "        0.11556409,  0.34924078,  0.63314235,  0.13338865,  0.95997417,\n",
       "        0.20283358,  0.30683768,  0.21860147, -0.28488544,  0.302402  ,\n",
       "        0.00477891, -0.16472505,  0.5784905 , -0.5578387 ,  0.555327  ,\n",
       "       -0.02437277, -0.62865025, -0.3149756 , -0.2510006 ,  0.2584461 ,\n",
       "       -0.40898165,  0.40648326, -0.02374619, -0.4059707 , -0.76116735,\n",
       "        0.45773026,  0.32588002, -0.56725925, -0.76087284, -0.23437585,\n",
       "       -0.23928988,  0.30740294, -0.4052821 , -0.28200647, -0.25921187,\n",
       "       -0.46740112, -0.06332638,  0.21706437, -0.3786054 , -0.23418869,\n",
       "       -0.22711867, -0.33917323,  0.13075432,  0.180928  ,  0.80321753,\n",
       "        0.25314912, -0.3439904 ,  0.55363506, -1.0387908 ,  0.00621789,\n",
       "       -0.23419784, -0.05006967,  0.7180267 , -0.07470898,  0.21499386,\n",
       "        0.3810609 ,  0.16886152,  0.3271383 ], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodel['bad']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "428\n",
      "251637\n",
      "(25000, 2560)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer_obj = Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(list)\n",
    "sequences = tokenizer_obj.texts_to_sequences(list)\n",
    "\n",
    "\n",
    "print(len(sequences[1])) #it works!!!!\n",
    "\n",
    "\n",
    "#pad sequences\n",
    "imdb_word_index = tokenizer_obj.word_index\n",
    "print(len(imdb_word_index))\n",
    "\n",
    "padded_list = pad_sequences(sequences, maxlen=2560)\n",
    "\n",
    "print(padded_list.shape)\n",
    "y_train = []\n",
    "for x in range(0,12500):\n",
    "    y_train.append(1)\n",
    "for x in range(0,12500):\n",
    "    y_train.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(251638, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_words = len(imdb_word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, 128))\n",
    "\n",
    "for word, i in imdb_word_index.items():\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print(embedding_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0722 13:56:54.296079 16348 deprecation_wrapper.py:119] From C:\\Users\\varun\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0722 13:56:54.376868 16348 deprecation_wrapper.py:119] From C:\\Users\\varun\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0722 13:56:56.019471 16348 deprecation_wrapper.py:119] From C:\\Users\\varun\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0722 13:56:56.135198 16348 deprecation_wrapper.py:119] From C:\\Users\\varun\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0722 13:56:56.145138 16348 deprecation.py:506] From C:\\Users\\varun\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0722 13:56:56.375521 16348 deprecation_wrapper.py:119] From C:\\Users\\varun\\Miniconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0722 13:56:56.399497 16348 deprecation_wrapper.py:119] From C:\\Users\\varun\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0722 13:56:56.408432 16348 deprecation.py:323] From C:\\Users\\varun\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2560, 128)         32209664  \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                15456     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 32,225,153\n",
      "Trainable params: 15,489\n",
      "Non-trainable params: 32,209,664\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build model!!!\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Embedding(num_words, \n",
    "                    128, \n",
    "                    embeddings_initializer=Constant(embedding_matrix),\n",
    "                    input_length=2560, \n",
    "                    trainable=False))\n",
    "\n",
    "model.add(GRU(units=32, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "25000/25000 [==============================] - ETA: 40:03 - loss: 0.7031 - acc: 0.51 - ETA: 26:54 - loss: 0.7075 - acc: 0.49 - ETA: 22:10 - loss: 0.7028 - acc: 0.48 - ETA: 19:23 - loss: 0.7004 - acc: 0.50 - ETA: 17:38 - loss: 0.7014 - acc: 0.49 - ETA: 16:17 - loss: 0.7005 - acc: 0.49 - ETA: 15:28 - loss: 0.6999 - acc: 0.50 - ETA: 14:47 - loss: 0.7003 - acc: 0.49 - ETA: 14:09 - loss: 0.6996 - acc: 0.50 - ETA: 13:36 - loss: 0.6993 - acc: 0.49 - ETA: 13:07 - loss: 0.6983 - acc: 0.50 - ETA: 12:47 - loss: 0.6975 - acc: 0.50 - ETA: 12:33 - loss: 0.6974 - acc: 0.50 - ETA: 12:14 - loss: 0.6965 - acc: 0.50 - ETA: 11:55 - loss: 0.6966 - acc: 0.50 - ETA: 11:38 - loss: 0.6957 - acc: 0.50 - ETA: 11:24 - loss: 0.6956 - acc: 0.50 - ETA: 11:10 - loss: 0.6952 - acc: 0.51 - ETA: 10:56 - loss: 0.6950 - acc: 0.51 - ETA: 10:42 - loss: 0.6950 - acc: 0.51 - ETA: 10:28 - loss: 0.6951 - acc: 0.51 - ETA: 10:16 - loss: 0.6947 - acc: 0.51 - ETA: 10:04 - loss: 0.6947 - acc: 0.51 - ETA: 9:53 - loss: 0.6940 - acc: 0.5169 - ETA: 9:43 - loss: 0.6938 - acc: 0.516 - ETA: 9:33 - loss: 0.6936 - acc: 0.518 - ETA: 9:22 - loss: 0.6934 - acc: 0.518 - ETA: 9:13 - loss: 0.6927 - acc: 0.521 - ETA: 9:03 - loss: 0.6927 - acc: 0.522 - ETA: 8:53 - loss: 0.6924 - acc: 0.523 - ETA: 8:43 - loss: 0.6923 - acc: 0.523 - ETA: 8:34 - loss: 0.6921 - acc: 0.523 - ETA: 8:25 - loss: 0.6920 - acc: 0.525 - ETA: 8:15 - loss: 0.6915 - acc: 0.527 - ETA: 8:06 - loss: 0.6915 - acc: 0.527 - ETA: 7:58 - loss: 0.6916 - acc: 0.526 - ETA: 7:50 - loss: 0.6912 - acc: 0.527 - ETA: 7:41 - loss: 0.6912 - acc: 0.526 - ETA: 7:32 - loss: 0.6912 - acc: 0.527 - ETA: 7:24 - loss: 0.6910 - acc: 0.527 - ETA: 7:16 - loss: 0.6908 - acc: 0.528 - ETA: 7:08 - loss: 0.6906 - acc: 0.529 - ETA: 7:00 - loss: 0.6901 - acc: 0.530 - ETA: 6:52 - loss: 0.6901 - acc: 0.530 - ETA: 6:44 - loss: 0.6899 - acc: 0.531 - ETA: 6:36 - loss: 0.6897 - acc: 0.532 - ETA: 6:28 - loss: 0.6899 - acc: 0.532 - ETA: 6:21 - loss: 0.6898 - acc: 0.532 - ETA: 6:14 - loss: 0.6897 - acc: 0.533 - ETA: 6:07 - loss: 0.6894 - acc: 0.534 - ETA: 5:59 - loss: 0.6891 - acc: 0.536 - ETA: 5:52 - loss: 0.6890 - acc: 0.536 - ETA: 5:45 - loss: 0.6888 - acc: 0.537 - ETA: 5:38 - loss: 0.6886 - acc: 0.537 - ETA: 5:30 - loss: 0.6883 - acc: 0.537 - ETA: 5:22 - loss: 0.6881 - acc: 0.538 - ETA: 5:15 - loss: 0.6880 - acc: 0.539 - ETA: 5:07 - loss: 0.6878 - acc: 0.539 - ETA: 4:59 - loss: 0.6877 - acc: 0.540 - ETA: 4:52 - loss: 0.6875 - acc: 0.540 - ETA: 4:44 - loss: 0.6872 - acc: 0.541 - ETA: 4:37 - loss: 0.6871 - acc: 0.541 - ETA: 4:30 - loss: 0.6869 - acc: 0.542 - ETA: 4:22 - loss: 0.6865 - acc: 0.543 - ETA: 4:15 - loss: 0.6864 - acc: 0.543 - ETA: 4:07 - loss: 0.6864 - acc: 0.543 - ETA: 4:00 - loss: 0.6863 - acc: 0.543 - ETA: 3:52 - loss: 0.6862 - acc: 0.543 - ETA: 3:44 - loss: 0.6861 - acc: 0.544 - ETA: 3:37 - loss: 0.6858 - acc: 0.545 - ETA: 3:29 - loss: 0.6857 - acc: 0.545 - ETA: 3:21 - loss: 0.6857 - acc: 0.545 - ETA: 3:13 - loss: 0.6855 - acc: 0.546 - ETA: 3:05 - loss: 0.6852 - acc: 0.546 - ETA: 2:57 - loss: 0.6851 - acc: 0.546 - ETA: 2:50 - loss: 0.6851 - acc: 0.547 - ETA: 2:42 - loss: 0.6849 - acc: 0.547 - ETA: 2:34 - loss: 0.6849 - acc: 0.547 - ETA: 2:26 - loss: 0.6846 - acc: 0.548 - ETA: 2:18 - loss: 0.6844 - acc: 0.549 - ETA: 2:10 - loss: 0.6842 - acc: 0.549 - ETA: 2:03 - loss: 0.6841 - acc: 0.550 - ETA: 1:55 - loss: 0.6841 - acc: 0.550 - ETA: 1:47 - loss: 0.6840 - acc: 0.550 - ETA: 1:39 - loss: 0.6839 - acc: 0.550 - ETA: 1:31 - loss: 0.6836 - acc: 0.551 - ETA: 1:23 - loss: 0.6833 - acc: 0.552 - ETA: 1:16 - loss: 0.6832 - acc: 0.552 - ETA: 1:08 - loss: 0.6830 - acc: 0.552 - ETA: 1:00 - loss: 0.6829 - acc: 0.553 - ETA: 52s - loss: 0.6828 - acc: 0.553 - ETA: 44s - loss: 0.6825 - acc: 0.55 - ETA: 36s - loss: 0.6823 - acc: 0.55 - ETA: 28s - loss: 0.6822 - acc: 0.55 - ETA: 21s - loss: 0.6820 - acc: 0.55 - ETA: 13s - loss: 0.6817 - acc: 0.55 - ETA: 5s - loss: 0.6815 - acc: 0.5559 - 788s 32ms/step - loss: 0.6814 - acc: 0.5560\n",
      "Epoch 2/5\n",
      " 6912/25000 [=======>......................] - ETA: 12:45 - loss: 0.6696 - acc: 0.58 - ETA: 12:59 - loss: 0.6607 - acc: 0.59 - ETA: 12:43 - loss: 0.6668 - acc: 0.59 - ETA: 12:50 - loss: 0.6665 - acc: 0.58 - ETA: 13:00 - loss: 0.6673 - acc: 0.58 - ETA: 13:08 - loss: 0.6667 - acc: 0.58 - ETA: 13:25 - loss: 0.6655 - acc: 0.59 - ETA: 13:42 - loss: 0.6647 - acc: 0.59 - ETA: 14:01 - loss: 0.6627 - acc: 0.59 - ETA: 14:15 - loss: 0.6635 - acc: 0.60 - ETA: 14:27 - loss: 0.6624 - acc: 0.59 - ETA: 14:35 - loss: 0.6602 - acc: 0.60 - ETA: 14:42 - loss: 0.6598 - acc: 0.60 - ETA: 14:43 - loss: 0.6583 - acc: 0.60 - ETA: 14:42 - loss: 0.6570 - acc: 0.61 - ETA: 14:35 - loss: 0.6563 - acc: 0.61 - ETA: 14:25 - loss: 0.6571 - acc: 0.61 - ETA: 14:13 - loss: 0.6564 - acc: 0.61 - ETA: 2:11:55 - loss: 0.6555 - acc: 0.61 - ETA: 2:04:20 - loss: 0.6550 - acc: 0.61 - ETA: 1:57:24 - loss: 0.6549 - acc: 0.61 - ETA: 1:51:02 - loss: 0.6533 - acc: 0.61 - ETA: 1:45:17 - loss: 0.6530 - acc: 0.61 - ETA: 1:39:58 - loss: 0.6537 - acc: 0.61 - ETA: 1:35:04 - loss: 0.6535 - acc: 0.61 - ETA: 1:30:33 - loss: 0.6524 - acc: 0.61 - ETA: 1:26:21 - loss: 0.6531 - acc: 0.6157"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-f0e69ee40393>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(padded_list, y_train, batch_size=256, epochs=5, verbose=1)\n",
    "model.save(\"my_W2V_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model \n",
    "model = load_model('my_W2V_model.h5')\n",
    "\n",
    "import keras\n",
    "\n",
    "\n",
    "sentence = input(\"Type in sentence.\")\n",
    "type(sentence)\n",
    "\n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "testIntArray = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word in imdb_word_index:\n",
    "        testIntArray.append(imdb_word_index[word])\n",
    "    \n",
    "\n",
    "finalTestArray = np.array([testIntArray])\n",
    "finalTestArray = keras.preprocessing.sequence.pad_sequences(finalTestArray, maxlen=256, value=0, padding='post')\n",
    "print(finalTestArray)\n",
    "\n",
    "model.predict(finalTestArray)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
